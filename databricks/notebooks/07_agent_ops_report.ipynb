{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5d859c-a28a-4e06-8876-a8b5aaada9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9cb602-17b5-477a-b5c1-2cd26341b8d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Table names\n",
    "metrics_table     = \"nlp_dev.agents.model_metrics\"\n",
    "confusion_table   = \"nlp_dev.gold_gold.fact_sentiment_confusion\"\n",
    "summary_table     = \"nlp_dev.gold_gold.fact_sentiment_summary\"\n",
    "\n",
    "ops_reports_table = \"nlp_dev.agents.ops_reports\"\n",
    "\n",
    "\n",
    "# Storage path for reports\n",
    "ops_reports_path = (\n",
    "    \"abfss://nlp@nlplakeadls001.dfs.core.windows.net/\"\n",
    "    \"agents/ops_reports\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488ee50d-2124-4745-a2ac-90514c3ce9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_metrics = spark.table(metrics_table)\n",
    "\n",
    "# Get the latest metrics row (by created_at)\n",
    "w = Window.orderBy(F.col(\"created_at\").desc())\n",
    "\n",
    "df_latest_metrics = (\n",
    "    df_metrics.withColumn(\"rn\", F.row_number().over(w))\n",
    "      .filter(\"rn = 1\")\n",
    "      .drop(\"rn\")\n",
    ")\n",
    "\n",
    "display(df_latest_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "923f76c7-1018-4178-82ee-d2c7a5f6f491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_conf = spark.table(confusion_table)\n",
    "df_sum  = spark.table(summary_table)\n",
    "\n",
    "display(df_conf)\n",
    "display(df_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48737c2d-0a42-475d-b43b-c3a8390927c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collect metrics row to driver as dict\n",
    "metrics = df_latest_metrics.limit(1).toPandas().to_dict(orient=\"records\")[0]\n",
    "\n",
    "# Collect confusion and summary as small JSON-friendly objects\n",
    "confusion_rows = df_conf.toPandas().to_dict(orient=\"records\")\n",
    "summary_rows   = df_sum.toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "context = {\n",
    "     \"metrics\": metrics,\n",
    "    \"confusion\": confusion_rows,\n",
    "    \"summary\": summary_rows\n",
    "}\n",
    "\n",
    "print(json.dumps(context, indent=2, default=str)[:2000])  # preview trimmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2714b0c2-d5cf-4b54-a03a-f16ba8acabf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_ops_summary(context: dict) -> dict:\n",
    "\n",
    "\n",
    "    m = context[\"metrics\"]\n",
    "    overall_acc = float(m.get(\"overall_accuracy\",0.0))\n",
    "\n",
    "    # basic flags\n",
    "    needs_retrain = overall_acc < 0.85\n",
    "    performance_tier = \"good\"\n",
    "\n",
    "    if overall_acc >= 0.9:\n",
    "        performance_tier = \"excellent\"\n",
    "    elif overall_acc < 0.8:\n",
    "        performance_tier = \"concerning\"\n",
    "\n",
    "    # Simple human-readable summary\n",
    "    summary_lines = []\n",
    "    summary_lines.append(\n",
    "            f\"Model '{m.get('model_name')}' (version {m.get('model_version')}) \"\n",
    "            f\"has overall accuracy of {overall_acc:.4f} on scope '{m.get('metric_scope')}'.\"\n",
    "    )\n",
    "    summary_lines.append(f\"Performance tier: {performance_tier}.\")\n",
    "    if needs_retrain:\n",
    "        summary_lines.append(\n",
    "            \"Accuracy is below 0.85. Consider retraining the model on more recent data.\"\n",
    "        )\n",
    "    else:\n",
    "        summary_lines.append(\n",
    "            \"Accuracy is above 0.85. No immediate retraining required.\"\n",
    "        )\n",
    "\n",
    "    summary_text = \" \".join(summary_lines)\n",
    "\n",
    "    flags = {\n",
    "        \"needs_retrain\": needs_retrain,\n",
    "        \"performance_tier\": performance_tier,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"summary_text\": summary_text,\n",
    "        \"flags\": flags,\n",
    "    }\n",
    "\n",
    "agent_output = build_ops_summary(context)\n",
    "print(agent_output[\"summary_text\"])\n",
    "print(agent_output[\"flags\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d46fd1-bba7-4dc6-b059-27d4009da3e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Build a clean, typed Row for the report\n",
    "report_row = Row(\n",
    "    report_id       = str(uuid.uuid4()),\n",
    "    model_name      = str(metrics.get(\"model_name\") or \"\"),\n",
    "    model_version   = int(metrics.get(\"model_version\") or 0),\n",
    "    run_id          = str(metrics.get(\"run_id\") or \"\"),\n",
    "    metric_scope    = str(metrics.get(\"metric_scope\") or \"\"),\n",
    "    overall_accuracy= float(metrics.get(\"overall_accuracy\") or 0.0),\n",
    "    summary_text    = agent_output[\"summary_text\"],\n",
    "    flags_json      = json.dumps(agent_output[\"flags\"]),\n",
    ")\n",
    "\n",
    "# Create a DataFrame with one row, and let Spark set generated_at\n",
    "df_report = (\n",
    "    spark.createDataFrame([report_row])\n",
    "         .withColumn(\"generated_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "display(df_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2837d85f-d547-45e7-8547-a3f18e0a5892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_report.write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .save(ops_reports_path)\n",
    ")\n",
    "print(\"Appended ops report to Delta path:\", ops_reports_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e74b7d-649f-471a-8948-d6523375cd5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS nlp_dev.agents.ops_reports\n",
    "USING DELTA\n",
    "LOCATION 'abfss://nlp@nlplakeadls001.dfs.core.windows.net/agents/ops_reports';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbac87a-5940-461e-b750-638d3faace2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM nlp_dev.agents.ops_reports\n",
    "ORDER BY generated_at DESC;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4707726522504688,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "07_agent_ops_report",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
